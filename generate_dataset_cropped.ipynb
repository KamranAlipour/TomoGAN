{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['s1']\n",
    "p_noisy = ['00','10','15','25'] # # 0% is GT (clean) and the rest are noisy inputs\n",
    "frame_count = 279 # number of frames per case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:  (1, 558, 1024, 1024, 3)\n",
      "frames:  (1, 558, 1024, 1024, 3)\n",
      "frames:  (1, 558, 1024, 1024, 3)\n",
      "frames:  (1, 558, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "#loaded_data = None\n",
    "for s in samples:\n",
    "    loaded_sample = None\n",
    "    for p in p_noisy:\n",
    "        loaded_frame = None\n",
    "        for i in range(frame_count):\n",
    "            for lr in [0,1]:\n",
    "                file_name = 'dataset/cropped/frames_split_1024/'+s+'_'+p+'_'+str(i+1).zfill(3)+'-'+str(lr)+'.jpg'\n",
    "                #print(file_name)\n",
    "                im = imageio.imread(file_name)\n",
    "                np_im = np.array(im)\n",
    "                np_im = np.expand_dims(np_im,axis = 0)\n",
    "                if (loaded_frame is None):\n",
    "                    loaded_frame = np_im\n",
    "                else:\n",
    "                    #print(np_im.shape,loaded_frame.shape)\n",
    "                    loaded_frame = np.append(loaded_frame,np_im,axis=0)\n",
    "        loaded_frame = np.expand_dims(loaded_frame,axis = 0)\n",
    "        print('frames: ',loaded_frame.shape)\n",
    "        hf = h5py.File('dataset/cropped/data_'+s+'_'+p+'_1024x1024.h5', 'w') # (frame,noise,width,height,channel)\n",
    "        hf.create_dataset('images', data=loaded_frame)\n",
    "        hf.close()\n",
    "    #if (loaded_filter is None):\n",
    "    #    loaded_filter = loaded_frame\n",
    "    #else:\n",
    "    #    loaded_filter = np.append(loaded_filter,loaded_frame,axis=0)\n",
    "    \"\"\"    \n",
    "    loaded_filter = np.expand_dims(loaded_filter,axis = 0)\n",
    "    print('filters: ',loaded_filter.shape)\n",
    "    hf = h5py.File('dataset/cropped/data_'+s+'_'+p+'_1024x1024.h5', 'w') # (frame,noise,width,height,channel)\n",
    "    hf.create_dataset('images', data=loaded_sample) \n",
    "    hf.close()\n",
    "    \"\"\"\n",
    "    #if (loaded_sample is None):\n",
    "    #    loaded_sample = loaded_filter\n",
    "    #else:\n",
    "    #    loaded_sample = np.append(loaded_sample,loaded_filter,axis=0)\n",
    "    #print(loaded_sample.shape)\n",
    "    #loaded_sample = np.expand_dims(loaded_sample,axis = 0)\n",
    "    #hf = h5py.File('dataset/cropped/data_'+s+'_1024x1024.h5', 'w') # (frame,noise,width,height,channel)\n",
    "    #hf.create_dataset('images', data=loaded_sample) \n",
    "    #hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 558, 1024, 1024, 3)\n",
      "(1, 558, 1024, 1024, 3)\n",
      "(1, 558, 1024, 1024, 3)\n",
      "(1, 558, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# train and test data only based on samples s1\n",
    "train_split = 0.8\n",
    "train_data = None\n",
    "test_data = None\n",
    "frame_count = 558 # split\n",
    "for s in ['s1']: # ignore s2,s3 for now\n",
    "    frame_range = np.arange(frame_count) # frame indices for all samples\n",
    "    np.random.shuffle(frame_range)\n",
    "    train_range = frame_range[:int(frame_count *  train_split)]\n",
    "    train_range = np.sort(train_range)\n",
    "    test_range = frame_range[int(frame_count *  train_split):]\n",
    "    test_range = np.sort(test_range)\n",
    "    for n in ['00','10','15','25']:\n",
    "        f = h5py.File(\"dataset/cropped/h5/data_\"+s+\"_\"+n+\"_1024x1024.h5\", \"r\")\n",
    "        all_data = f['images']\n",
    "        train_data = np.squeeze(all_data, axis=0)[train_range,:,:,0]\n",
    "        test_data = np.squeeze(all_data, axis=0)[test_range,:,:,0]\n",
    "        #all_data = np.transpose(all_data,(0,3,1,2)) # (frame,noise,channel,height,width)\n",
    "        print(all_data.shape)\n",
    "        f.close()\n",
    "        if n == '00':\n",
    "            hf = h5py.File('dataset/cropped/h5/clean4train_'+s+'.h5', 'w')\n",
    "            hf.create_dataset('images', data=train_data)\n",
    "            hf.close()\n",
    "            hf = h5py.File('dataset/cropped/h5/clean4test_'+s+'.h5', 'w')\n",
    "            hf.create_dataset('images', data=test_data)\n",
    "            hf.close()\n",
    "        else:\n",
    "            hf = h5py.File('dataset/cropped/h5/noisy4train_n'+n+'_'+s+'.h5', 'w')\n",
    "            hf.create_dataset('images', data=train_data)\n",
    "            hf.close()\n",
    "            hf = h5py.File('dataset/cropped/h5/noisy4test_n'+n+'_'+s+'.h5', 'w')\n",
    "            hf.create_dataset('images', data=test_data)\n",
    "            hf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/cropped/h5/clean4train_s2.h5\n"
     ]
    }
   ],
   "source": [
    "for tt in ['noisy']:\n",
    "    for nn in ['10','15','25']:\n",
    "        #print('dataset/cropped/'+tt+'4train_n'+nn+'_s2.h5')\n",
    "        f_s1 = h5py.File('dataset/cropped/h5/'+tt+'4train_n'+nn+'_s1.h5', \"r\")\n",
    "        f_s2 = h5py.File('dataset/cropped/h5/'+tt+'4train_n'+nn+'_s2.h5', \"r\")\n",
    "        f_s3 = h5py.File('dataset/cropped/h5/'+tt+'4train_n'+nn+'_s3.h5', \"r\")\n",
    "        s1_data = f_s1['images']\n",
    "        s2_data = f_s2['images']\n",
    "        s3_data = f_s3['images']\n",
    "        s1_s2_data = np.append(s1_data,s2_data,axis=0)\n",
    "        s1_s2_s3_data = np.append(s1_s2_data,s3_data,axis=0)\n",
    "        hf = h5py.File('dataset/cropped/h5/'+tt+'4train_n'+nn+'_s1_s2_s3.h5', 'w')\n",
    "        hf.create_dataset('images', data=s1_s2_s3_data)\n",
    "        hf.close()\n",
    "        f_s1.close()\n",
    "        f_s2.close()\n",
    "        f_s3.close()\n",
    "    \n",
    "for tt in ['clean']:\n",
    "    print('dataset/cropped/h5/'+tt+'4train_s2.h5')\n",
    "    f_s1 = h5py.File('dataset/cropped/h5/'+tt+'4train_s1.h5', \"r\")\n",
    "    f_s2 = h5py.File('dataset/cropped/h5/'+tt+'4train_s2.h5', \"r\")\n",
    "    f_s3 = h5py.File('dataset/cropped/h5/'+tt+'4train_s3.h5', \"r\")\n",
    "    s1_data = f_s1['images']\n",
    "    s2_data = f_s2['images']\n",
    "    s3_data = f_s3['images']\n",
    "    s1_s2_data = np.append(s1_data,s2_data,axis=0)\n",
    "    s1_s2_s3_data = np.append(s1_s2_data,s3_data,axis=0)\n",
    "    hf = h5py.File('dataset/cropped/h5/'+tt+'4train_s1_s2_s3.h5', 'w')\n",
    "    hf.create_dataset('images', data=s1_s2_s3_data)\n",
    "    hf.close()\n",
    "    f_s1.close()\n",
    "    f_s2.close()\n",
    "    f_s3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt in ['noisy']:\n",
    "    for nn in ['10','15','25']:\n",
    "        #print('dataset/cropped/'+tt+'4test_n'+nn+'_s2.h5')\n",
    "        f_s1 = h5py.File('dataset/cropped/h5/'+tt+'4test_n'+nn+'_s1.h5', \"r\")\n",
    "        f_s2 = h5py.File('dataset/cropped/h5/'+tt+'4test_n'+nn+'_s2.h5', \"r\")\n",
    "        f_s3 = h5py.File('dataset/cropped/h5/'+tt+'4test_n'+nn+'_s3.h5', \"r\")\n",
    "        s1_data = f_s1['images']\n",
    "        s2_data = f_s2['images']\n",
    "        s3_data = f_s3['images']\n",
    "        s1_s2_data = np.append(s1_data,s2_data,axis=0)\n",
    "        s1_s2_s3_data = np.append(s1_s2_data,s3_data,axis=0)\n",
    "        hf = h5py.File('dataset/cropped/h5/'+tt+'4test_n'+nn+'_s1_s2_s3.h5', 'w')\n",
    "        hf.create_dataset('images', data=s1_s2_s3_data)\n",
    "        hf.close()\n",
    "        f_s1.close()\n",
    "        f_s2.close()\n",
    "        f_s3.close()\n",
    "    \n",
    "for tt in ['clean']:\n",
    "    #print('dataset/cropped/'+tt+'4test_s2.h5')\n",
    "    f_s1 = h5py.File('dataset/cropped/h5/'+tt+'4test_s1.h5', \"r\")\n",
    "    f_s2 = h5py.File('dataset/cropped/h5/'+tt+'4test_s2.h5', \"r\")\n",
    "    f_s3 = h5py.File('dataset/cropped/h5/'+tt+'4test_s3.h5', \"r\")\n",
    "    s1_data = f_s1['images']\n",
    "    s2_data = f_s2['images']\n",
    "    s3_data = f_s3['images']\n",
    "    s1_s2_data = np.append(s1_data,s2_data,axis=0)\n",
    "    s1_s2_s3_data = np.append(s1_s2_data,s3_data,axis=0)\n",
    "    hf = h5py.File('dataset/cropped/h5/'+tt+'4test_s1_s2_s3.h5', 'w')\n",
    "    hf.create_dataset('images', data=s1_s2_s3_data)\n",
    "    hf.close()\n",
    "    f_s1.close()\n",
    "    f_s2.close()\n",
    "    f_s3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
